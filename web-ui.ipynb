{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36760,
     "status": "ok",
     "timestamp": 1756989911264,
     "user": {
      "displayName": "Nakul Chamariya",
      "userId": "14936028262101279399"
     },
     "user_tz": -330
    },
    "id": "Y3X4W5Ytx1TH",
    "outputId": "3e7cc43b-c0c5-46c3-dabf-3d38a40604e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 26239,
     "status": "ok",
     "timestamp": 1756989937499,
     "user": {
      "displayName": "Nakul Chamariya",
      "userId": "14936028262101279399"
     },
     "user_tz": -330
    },
    "id": "oIz_iYcmyVDp"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/coreference/coreference_model.pth ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9032,
     "status": "ok",
     "timestamp": 1756989946529,
     "user": {
      "displayName": "Nakul Chamariya",
      "userId": "14936028262101279399"
     },
     "user_tz": -330
    },
    "id": "Kiza_Pslxvnj",
    "outputId": "7fe02416-5857-479f-ce1d-2bebc98162d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
      "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyngrok, pydeck, streamlit\n",
      "Successfully installed pydeck-0.9.1 pyngrok-7.3.0 streamlit-1.49.1\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1756990373967,
     "user": {
      "displayName": "Nakul Chamariya",
      "userId": "14936028262101279399"
     },
     "user_tz": -330
    },
    "id": "qhs8YjSEyHKQ",
    "outputId": "405122d8-afa5-44fe-a07d-f127df5af253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.patches import FancyBboxPatch, ConnectionPatch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Define the model class (same as training)\n",
    "class CoreferenceModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(CoreferenceModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.classifier(output)\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    \"\"\"Load the trained coreference model\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load('/content/coreference_model.pth', weights_only=False)\n",
    "        model = CoreferenceModel(checkpoint['model_name']).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        tokenizer = checkpoint['tokenizer']\n",
    "        model.eval()\n",
    "        return model, tokenizer, device\n",
    "    except Exception as e:\n",
    "        st.error(e)\n",
    "        return None, None, None\n",
    "\n",
    "def predict_coreference(model, tokenizer, device, token1, token2):\n",
    "    \"\"\"Predict if two tokens are coreferent\"\"\"\n",
    "    if model is None:\n",
    "        return 0.5\n",
    "\n",
    "    text = f\"{token1} [SEP] {token2}\"\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        coreference_prob = probabilities[0][1].item()  # Probability of being coreferent\n",
    "\n",
    "    return coreference_prob\n",
    "\n",
    "def create_clusters_from_text(text, model, tokenizer, device, threshold=0.7):\n",
    "    \"\"\"Create coreference clusters from input text\"\"\"\n",
    "    # Simple tokenization (split by spaces and punctuation)\n",
    "    import re\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "    if not tokens:\n",
    "        return [], {}\n",
    "\n",
    "    # Create token mapping\n",
    "    token_to_id = {token: f\"T{i+1}\" for i, token in enumerate(set(tokens))}\n",
    "\n",
    "    # Get all unique tokens with their positions\n",
    "    unique_tokens = list(set(tokens))\n",
    "    token_positions = {}\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in token_positions:\n",
    "            token_positions[token] = []\n",
    "        token_positions[token].append(i)\n",
    "\n",
    "    # Predict coreference for all pairs\n",
    "    clusters = []\n",
    "    used_tokens = set()\n",
    "\n",
    "    for i, token1 in enumerate(unique_tokens):\n",
    "        if token1 in used_tokens:\n",
    "            continue\n",
    "\n",
    "        current_cluster = [token1]\n",
    "        used_tokens.add(token1)\n",
    "\n",
    "        for j, token2 in enumerate(unique_tokens[i+1:], i+1):\n",
    "            if token2 in used_tokens:\n",
    "                continue\n",
    "\n",
    "            prob = predict_coreference(model, tokenizer, device, token1, token2)\n",
    "\n",
    "            if prob > threshold:\n",
    "                current_cluster.append(token2)\n",
    "                used_tokens.add(token2)\n",
    "\n",
    "        if len(current_cluster) > 1:\n",
    "            clusters.append(current_cluster)\n",
    "\n",
    "    return clusters, token_to_id\n",
    "\n",
    "def create_cluster_table(clusters, token_to_id):\n",
    "    \"\"\"Create a DataFrame for cluster visualization\"\"\"\n",
    "    data = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster_id = f\"C{i+1}\"\n",
    "        for token in cluster:\n",
    "            data.append({\n",
    "                'Cluster ID': cluster_id,\n",
    "                'Token': token,\n",
    "                'Token ID': token_to_id[token]\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_network_graph(clusters, token_to_id):\n",
    "    \"\"\"Create network graph visualization\"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes and edges\n",
    "    colors = px.colors.qualitative.Set3\n",
    "    cluster_colors = {}\n",
    "\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        color = colors[i % len(colors)]\n",
    "        cluster_colors[f\"C{i+1}\"] = color\n",
    "\n",
    "        # Add nodes\n",
    "        for token in cluster:\n",
    "            G.add_node(token_to_id[token], cluster=f\"C{i+1}\")\n",
    "\n",
    "        # Add edges within cluster\n",
    "        for j, token1 in enumerate(cluster):\n",
    "            for token2 in cluster[j+1:]:\n",
    "                G.add_edge(token_to_id[token1], token_to_id[token2])\n",
    "\n",
    "    # Get layout\n",
    "    pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "\n",
    "    # Add edges\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=2, color='lightgray'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    ))\n",
    "\n",
    "    # Add nodes\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster_id = f\"C{i+1}\"\n",
    "        color = cluster_colors[cluster_id]\n",
    "\n",
    "        node_x = []\n",
    "        node_y = []\n",
    "        node_text = []\n",
    "        hover_text = []\n",
    "\n",
    "        for token in cluster:\n",
    "            token_id = token_to_id[token]\n",
    "            x, y = pos[token_id]\n",
    "            node_x.append(x)\n",
    "            node_y.append(y)\n",
    "            node_text.append(token_id)\n",
    "            hover_text.append(f\"Token: {token}<br>ID: {token_id}<br>Cluster: {cluster_id}\")\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=node_x, y=node_y,\n",
    "            mode='markers+text',\n",
    "            text=node_text,\n",
    "            textposition='middle center',\n",
    "            hovertext=hover_text,\n",
    "            hoverinfo='text',\n",
    "            marker=dict(\n",
    "                size=30,\n",
    "                color=color,\n",
    "                line=dict(width=2, color='black')\n",
    "            ),\n",
    "            name=cluster_id\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Coreference Clusters Network\",\n",
    "        showlegend=True,\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=20,l=5,r=5,t=40),\n",
    "        annotations=[ dict(\n",
    "            text=\"Hover over nodes to see original tokens\",\n",
    "            showarrow=False,\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.005, y=-0.002,\n",
    "            xanchor='left', yanchor='bottom',\n",
    "            font=dict(color='gray', size=12)\n",
    "        )],\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        plot_bgcolor='white'\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_arc_diagram(text, clusters, token_to_id):\n",
    "    \"\"\"Create arc-based visualization showing coreference relationships\"\"\"\n",
    "    # Simple tokenization\n",
    "    import re\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "    if not tokens or not clusters:\n",
    "        return None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "    # Create a mapping from token to its ID for display\n",
    "    token_display_ids = []\n",
    "    for token in tokens:\n",
    "        token_display_ids.append(token_to_id.get(token, token))\n",
    "\n",
    "    # Position tokens along x-axis (base line)\n",
    "    x_positions = np.arange(len(tokens))\n",
    "    y_base = 0\n",
    "\n",
    "    # Colors for different clusters\n",
    "    colors = ['#2E8B57', '#4682B4', '#DC143C', '#FF8C00', '#9932CC', '#008B8B', '#B22222', '#228B22']\n",
    "\n",
    "    # Draw the base line\n",
    "    ax.axhline(y=y_base, color='black', linewidth=2, alpha=0.8)\n",
    "\n",
    "    # Plot token IDs on the base line\n",
    "    for i, token_id in enumerate(token_display_ids):\n",
    "        # Draw vertical tick mark\n",
    "        ax.plot([i, i], [y_base-0.05, y_base+0.05], 'k-', linewidth=2)\n",
    "\n",
    "        # Add token ID below the line\n",
    "        ax.text(i, y_base-0.15, str(token_id), ha='center', va='top',\n",
    "               fontsize=12, weight='bold')\n",
    "\n",
    "    # Draw arcs for coreference relationships\n",
    "    max_height = 0\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        color = colors[cluster_idx % len(colors)]\n",
    "\n",
    "        # Find all positions of tokens in this cluster\n",
    "        positions = []\n",
    "        for token in cluster:\n",
    "            for i, t in enumerate(tokens):\n",
    "                if t == token:\n",
    "                    positions.append(i)\n",
    "\n",
    "        positions = sorted(set(positions))  # Remove duplicates and sort\n",
    "\n",
    "        if len(positions) > 1:\n",
    "            # For each cluster, connect all pairs with arcs\n",
    "            # But to avoid clutter, we'll connect in a chain-like manner\n",
    "            for i in range(len(positions) - 1):\n",
    "                start_pos = positions[i]\n",
    "                end_pos = positions[i + 1]\n",
    "\n",
    "                # Calculate arc parameters\n",
    "                center_x = (start_pos + end_pos) / 2\n",
    "                width = end_pos - start_pos\n",
    "\n",
    "                # Arc height increases with distance and cluster index for layering\n",
    "                base_height = 0.3 + (width * 0.1)\n",
    "                arc_height = base_height + (cluster_idx * 0.2)\n",
    "                max_height = max(max_height, arc_height)\n",
    "\n",
    "                # Create semi-circle arc\n",
    "                theta = np.linspace(0, np.pi, 100)\n",
    "                arc_x = center_x + (width/2) * np.cos(theta)\n",
    "                arc_y = y_base + arc_height * np.sin(theta)\n",
    "\n",
    "                # Draw the arc\n",
    "                ax.plot(arc_x, arc_y, color=color, linewidth=3, alpha=0.8)\n",
    "\n",
    "                # Add small vertical lines at connection points\n",
    "                ax.plot([start_pos, start_pos], [y_base, y_base + 0.1],\n",
    "                       color=color, linewidth=3, alpha=0.8)\n",
    "                ax.plot([end_pos, end_pos], [y_base, y_base + 0.1],\n",
    "                       color=color, linewidth=3, alpha=0.8)\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_xlim(-0.5, len(tokens) - 0.5)\n",
    "    ax.set_ylim(-0.3, max_height + 0.3)\n",
    "\n",
    "    # Remove all spines and ticks except for custom elements\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Add title\n",
    "    ax.set_title('Coreference Resolution - Arc Diagram', fontsize=16, weight='bold', pad=20)\n",
    "\n",
    "    # Add legend if there are clusters\n",
    "    if clusters:\n",
    "        legend_elements = []\n",
    "        for i in range(len(clusters)):\n",
    "            color = colors[i % len(colors)]\n",
    "            legend_elements.append(plt.Line2D([0], [0], color=color, lw=3,\n",
    "                                            label=f'Cluster C{i+1}'))\n",
    "        ax.legend(handles=legend_elements, loc='upper right', frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_word_cluster_table(clusters, word_to_id):\n",
    "    \"\"\"Create a DataFrame for word cluster visualization\"\"\"\n",
    "    data = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster_id = f\"C{i+1}\"\n",
    "        for word in cluster:\n",
    "            data.append({\n",
    "                'Cluster ID': cluster_id,\n",
    "                'Word': word,\n",
    "                'Word ID': word_to_id[word]\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def extract_words_from_tamil_text(text):\n",
    "    \"\"\"Extract meaningful words from Tamil text, handling Tamil script properly\"\"\"\n",
    "    # Remove punctuation and split by spaces\n",
    "    # This regex preserves Tamil characters and removes common punctuation\n",
    "    words = re.findall(r'[\\u0B80-\\u0BFF]+|[a-zA-Z]+', text)\n",
    "\n",
    "    # Filter out very short words (single characters) that might be punctuation\n",
    "    words = [word.strip() for word in words if len(word.strip()) > 1]\n",
    "\n",
    "    return words\n",
    "\n",
    "def predict_coreference(model, tokenizer, device, word1, word2):\n",
    "    \"\"\"Predict if two words are coreferent\"\"\"\n",
    "    if model is None:\n",
    "        # Fallback: simple string similarity for demo\n",
    "        if word1.lower() == word2.lower():\n",
    "            return 0.9\n",
    "        elif word1.lower() in word2.lower() or word2.lower() in word1.lower():\n",
    "            return 0.6\n",
    "        else:\n",
    "            return 0.3\n",
    "\n",
    "    # Format input as the model expects\n",
    "    text = f\"{word1} [SEP] {word2}\"\n",
    "\n",
    "    try:\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            coreference_prob = probabilities[0][1].item()  # Probability of being coreferent\n",
    "\n",
    "        return coreference_prob\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting coreference: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def create_word_clusters_from_tamil_sentence(text, model, tokenizer, device, threshold=0.7):\n",
    "    \"\"\"Create coreference clusters from Tamil sentence input - returns word clusters\"\"\"\n",
    "\n",
    "    # Extract words from the text\n",
    "    words = extract_words_from_tamil_text(text)\n",
    "\n",
    "    if not words:\n",
    "        return [], {}\n",
    "\n",
    "    print(f\"Extracted words: {words}\")\n",
    "\n",
    "    # Create word mapping\n",
    "    unique_words = list(set(words))\n",
    "    word_to_id = {word: f\"W{i+1}\" for i, word in enumerate(unique_words)}\n",
    "\n",
    "    # Track word positions in original text\n",
    "    word_positions = defaultdict(list)\n",
    "    for i, word in enumerate(words):\n",
    "        word_positions[word].append(i)\n",
    "\n",
    "    # Create clusters using coreference prediction\n",
    "    clusters = []\n",
    "    used_words = set()\n",
    "\n",
    "    for i, word1 in enumerate(unique_words):\n",
    "        if word1 in used_words:\n",
    "            continue\n",
    "\n",
    "        current_cluster = [word1]\n",
    "        used_words.add(word1)\n",
    "\n",
    "        for j, word2 in enumerate(unique_words[i+1:], i+1):\n",
    "            if word2 in used_words:\n",
    "                continue\n",
    "\n",
    "            # Predict coreference between words\n",
    "            prob = predict_coreference(model, tokenizer, device, word1, word2)\n",
    "            # print(f\"Coreference probability between '{word1}' and '{word2}': {prob:.3f}\")\n",
    "\n",
    "            if prob > threshold:\n",
    "                current_cluster.append(word2)\n",
    "                used_words.add(word2)\n",
    "\n",
    "        # Only keep clusters with more than one word\n",
    "        if len(current_cluster) > 1:\n",
    "            clusters.append(current_cluster)\n",
    "\n",
    "    return clusters, word_to_id\n",
    "\n",
    "def get_coreference_clusters_tamil_words(text, model=None, tokenizer=None, device=None, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Main function: Given a Tamil sentence, return word coreference clusters.\n",
    "    Returns both clusters and a DataFrame for visualization.\n",
    "\n",
    "    Args:\n",
    "        text (str): Tamil sentence\n",
    "        model: Trained coreference model (optional)\n",
    "        tokenizer: Model tokenizer (optional)\n",
    "        device: PyTorch device (optional)\n",
    "        threshold (float): Coreference probability threshold\n",
    "\n",
    "    Returns:\n",
    "        clusters (list): List of word clusters\n",
    "        cluster_df (DataFrame): DataFrame for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure input is a valid string\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return [], pd.DataFrame()\n",
    "\n",
    "    print(f\"Input text: {text}\")\n",
    "\n",
    "    # Set default device if not provided\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Run cluster creation\n",
    "    clusters, word_to_id = create_word_clusters_from_tamil_sentence(\n",
    "        text=text,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        threshold=threshold\n",
    "    )\n",
    "\n",
    "    # Build DataFrame for visualization\n",
    "    if clusters:\n",
    "        cluster_df = create_word_cluster_table(clusters, word_to_id)\n",
    "    else:\n",
    "        cluster_df = pd.DataFrame(columns=['Cluster ID', 'Word', 'Word ID'])\n",
    "\n",
    "    return clusters, cluster_df\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(\n",
    "        page_title=\"Coreference Resolution\",\n",
    "        page_icon=\"üîó\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "\n",
    "    st.title(\"üîó Coreference Resolution System\")\n",
    "    st.markdown(\"Enter text to identify coreference clusters and visualize relationships between entities.\")\n",
    "\n",
    "    # Load model\n",
    "    model, tokenizer, device = load_model()\n",
    "\n",
    "    if model is None:\n",
    "        st.warning(\"Model not loaded. Running in demo mode with sample predictions.\")\n",
    "\n",
    "    # Input section\n",
    "    st.header(\"Input Text\")\n",
    "    input_text = st.text_area(\n",
    "        \"Enter your text here:\",\n",
    "        height=150,\n",
    "        placeholder=\"Enter text containing entities that might refer to the same thing...\"\n",
    "    )\n",
    "\n",
    "    # Threshold slider\n",
    "    threshold = st.slider(\"Coreference Threshold\", 0.1, 0.9, 0.7, 0.1)\n",
    "\n",
    "    if st.button(\"Analyze Coreference\", type=\"primary\"):\n",
    "        if input_text.strip():\n",
    "            with st.spinner(\"Analyzing coreference relationships...\"):\n",
    "                # Create clusters\n",
    "                clusters, cluster_df = get_coreference_clusters_tamil_words(\n",
    "                    text=input_text,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    device=device,\n",
    "                    threshold=0.6  # Lower threshold for demo\n",
    "                )\n",
    "\n",
    "\n",
    "                if clusters:\n",
    "                    st.success(f\"Found {len(clusters)} coreference clusters!\")\n",
    "\n",
    "                    st.header(\"üìä Cluster Table\")\n",
    "                    st.dataframe(cluster_df, use_container_width=True)\n",
    "\n",
    "\n",
    "                    # Cluster details\n",
    "                    st.header(\"üìù Cluster Details\")\n",
    "                    for i, cluster in enumerate(clusters):\n",
    "                        with st.expander(f\"Cluster C{i+1} - {len(cluster)} tokens\"):\n",
    "                            st.write(f\"  Cluster {i}: {cluster}\")\n",
    "\n",
    "                else:\n",
    "                    st.info(\"No coreference clusters found. Try lowering the threshold or using different text.\")\n",
    "\n",
    "        else:\n",
    "            st.error(\"Please enter some text to analyze.\")\n",
    "\n",
    "    # Sample texts\n",
    "    # st.header(\"üí° Sample Texts\")\n",
    "    # col1, col2 = st.columns(2)\n",
    "\n",
    "    # with col1:\n",
    "    #     if st.button(\"Load Sample 1\"):\n",
    "    #         sample1 = \"John went to the store. He bought some milk. The man was happy with his purchase.\"\n",
    "    #         st.session_state['sample_text'] = sample1\n",
    "\n",
    "    # with col2:\n",
    "    #     if st.button(\"Load Sample 2\"):\n",
    "    #         sample2 = \"Mary is a teacher. She works at the school. The woman loves her job very much.\"\n",
    "    #         st.session_state['sample_text'] = sample2\n",
    "\n",
    "    # # Load sample text if button was clicked\n",
    "    # if 'sample_text' in st.session_state:\n",
    "    #     st.text_area(\"Sample loaded:\", value=st.session_state['sample_text'], key=\"sample_display\")\n",
    "    #     if st.button(\"Use This Sample\"):\n",
    "    #         st.session_state['input_text'] = st.session_state['sample_text']\n",
    "    #         st.rerun()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 955,
     "status": "ok",
     "timestamp": 1756990375387,
     "user": {
      "displayName": "Nakul Chamariya",
      "userId": "14936028262101279399"
     },
     "user_tz": -330
    },
    "id": "kj5sa7G6yIgo",
    "outputId": "d8e0b335-1ce9-4ac5-b455-bcf66692b674"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://8069cf7895b5.ngrok-free.app'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "ngrok_key = \"2vOa3qpFVL6rc9bgTeWIrI3EG0i_4YHxrqQsdfmka7ryesrY9\"\n",
    "port = 8501\n",
    "\n",
    "ngrok.set_auth_token(ngrok_key)\n",
    "ngrok.connect(port).public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN8my2SDyJ4p"
   },
   "outputs": [],
   "source": [
    "!rm -rf logs.txt && streamlit run app.py &>/content/logs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WE5be3dyyZ0b"
   },
   "outputs": [],
   "source": [
    "# tamil_samples = {\n",
    "#     \"Tamil Sample 1\": \"\"\"\n",
    "#     ‡Æâ‡Æ©‡Øç‡Æ©‡Øà‡Æ™‡Øç ‡Æ™‡Ææ‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡Ææ‡Æ≤‡Øç ‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡Æµ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà‡ÆØ‡Ææ ? ‡Æ®‡ØÄ ‡Æ§‡Æ™‡Øç‡Æ™‡Æø ‡Æì‡Æü‡Æø ‡Æí‡Æ≥‡Æø‡Æ®‡Øç‡Æ§‡ØÅ ‡Æï‡Øä‡Æ≥‡Øç‡Æ≥ ‡Æµ‡Æ®‡Øç‡Æ§‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æµ‡Æ©‡Øç ‡Æé‡Æ©‡Øç‡Æ±‡ØÅ ‡Æ®‡Øá‡Æ±‡Øç‡Æ±‡Øà‡Æï‡Øç‡Æï‡Øá ‡Æä‡Æï‡Æø‡Æ§‡Øç‡Æ§‡Øá‡Æ©‡Øç .\n",
    "#     \"\"\",\n",
    "\n",
    "#     \"Tamil Sample 2\": \"\"\"\n",
    "#     ‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ‡Æö‡Øç ‡Æö‡ØÜ‡Æ©‡Øç‡Æ±‡Ææ‡Æ©‡Øç . ‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ®‡Æ≤‡Øç‡Æ≤ ‡ÆÆ‡Ææ‡Æ£‡Æµ‡Æ©‡Øç . ‡Æ™‡Øà‡ÆØ‡Æ©‡Øç ‡Æ™‡Æü‡Æø‡Æ™‡Øç‡Æ™‡Æø‡Æ≤‡Øç ‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§‡ØÅ ‡Æµ‡Æø‡Æ≥‡Æô‡Øç‡Æï‡ØÅ‡Æï‡Æø‡Æ±‡Ææ‡Æ©‡Øç .\n",
    "#     \"\"\",\n",
    "\n",
    "#     \"Tamil Sample 3\": \"\"\"\n",
    "#     ‡Æ™‡ØÜ‡Æ£‡Øç ‡Æï‡Æü‡Øà‡Æï‡Øç‡Æï‡ØÅ‡Æö‡Øç ‡Æö‡ØÜ‡Æ©‡Øç‡Æ±‡Ææ‡Æ≥‡Øç . ‡ÆÖ‡Æµ‡Æ≥‡Øç ‡Æ™‡Æ¥‡ÆÆ‡Øç ‡Æµ‡Ææ‡Æô‡Øç‡Æï‡Æø‡Æ©‡Ææ‡Æ≥‡Øç . ‡Æ™‡ØÜ‡Æ£‡Øç‡ÆÆ‡Æ£‡Æø ‡ÆÆ‡Æï‡Æø‡Æ¥‡Øç‡Æö‡Øç‡Æö‡Æø‡ÆØ‡Ææ‡Æï ‡Æá‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡Ææ‡Æ≥‡Øç .\n",
    "#     \"\"\"\n",
    "# }\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOXkFPApnr1Syw92ob0F/bG",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
